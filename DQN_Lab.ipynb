{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_4QNqjGfxNK",
        "outputId": "d099a312-668c-4151-9a9c-bc608ff60e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB_fUWLbjtQU",
        "outputId": "8a067f78-e770-4042-be36-dee5772607ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dqn_lab\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/dqn_lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0LMI-WLj03N",
        "outputId": "84fc5054-b645-4107-8331-364c5054f143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'DQN_Lab (1).ipynb'   DQN_Lab.ipynb   req.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzrvZPyZj6r4"
      },
      "outputs": [],
      "source": [
        "!pip install -r req.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-CYFrpOf_lf"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RbYwSS1Nf4NJ"
      },
      "outputs": [],
      "source": [
        "from gym import spaces\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xZk-5a6tf4FA"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic implementation of a Deep Q-Network. The architecture is the same as that described in the\n",
        "    Nature DQN paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete):\n",
        "        \"\"\"\n",
        "        Initialise the DQN\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param action_space: the action space of the environment\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert (\n",
        "            type(observation_space) == spaces.Box\n",
        "        ), \"observation_space must be of type Box\"\n",
        "        assert (\n",
        "            len(observation_space.shape) == 3\n",
        "        ), \"observation space must have the form channels x width x height\"\n",
        "        assert (\n",
        "            type(action_space) == spaces.Discrete\n",
        "        ), \"action_space must be of type Discrete\"\n",
        "\n",
        "        # TODO Implement DQN Network\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=observation_space.shape[0], out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.Flatten(1)\n",
        "            #nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.fully_connected = nn.Sequential(\n",
        "            nn.Linear(in_features=3136 , out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=action_space.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.model(x).view(x.size()[0],-1)\n",
        "        return self.fully_connected(conv_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5PCanZagIhB"
      },
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FsAMK0HYf34R"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones),\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J-DKlrTgEID"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3AUt0bj_f4DT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O2-aNTNqf4Bp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        use_double_dqn,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        gamma,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Initialise agent's networks, optimiser and replay buffer\n",
        "        self.policy = DQN(observation_space, action_space).to(device)\n",
        "        self.target = DQN(observation_space, action_space).to(device)\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        #replay memory\n",
        "        self.replay_memory = replay_buffer\n",
        "        self.use_ddqn = use_double_dqn\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(lr), lr=self.lr)\n",
        "        self.update_target_network()\n",
        "        \n",
        "\n",
        "        #raise NotImplementedError\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        #   Optimise the TD-error over a single minibatch of transitions\n",
        "        #   Sample the minibatch from the replay-memory\n",
        "        #   using done (as a float) instead of if statement\n",
        "        #   return loss\n",
        "        \n",
        "        states, actions, rewards, nxt_states, done = self.replay_memory.sample(self.batch_size)\n",
        "        states = np.array(states, dtype=np.float64) / 255.0\n",
        "        nxt_states = np.array(states, dtype=np.float64) / 255.0\n",
        "        states = torch.from_numpy(np.asarray(states)).long().to(device)\n",
        "        actions = torch.from_numpy(np.asarray(actions)).long().to(device)\n",
        "        rewards = torch.from_numpy(np.asarray(rewards)).float().to(device)\n",
        "        nxt_states = torch.from_numpy(np.asarray(actions)).float().to(device)\n",
        "        done = torch.from_numpy(np.asarray(done)).float().to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            if self.use_ddqn:\n",
        "                _, max_nxt_action = self.policy(nxt_states).max(1)\n",
        "                max_nxt_q = torch.max(self.target(nxt_states),1)[0].detach()\n",
        "            else:\n",
        "                nxt_q = self.target(nxt_states)\n",
        "                max_nxt_q, _ = nxt_q.max(1)\n",
        "                \n",
        "        idx = np.arange(self.batch_size)\n",
        "        input_q = (self.policy(states)[idx, actions]).double()\n",
        "        target_q = torch.max(self.target(nxt_states),1)[0].detach()\n",
        "        \n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        target_q[done] = 0.0\n",
        "        t_q = (rewards + (1 - done) * self.gamma * max_nxt_q).double() #torch.max(self.target_q(nxt_states),1)[0].detach()\n",
        "        loss = F.smooth_l1_loss(input_q, t_q)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "        #raise NotImplementedError\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # TODO update target_network parameters with policy_network parameters\n",
        "        self.target.load_state_dict(self.policy.state_dict())\n",
        "        #raise NotImplementedError\n",
        "\n",
        "    def act(self, state: np.ndarray):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # TODO Select action greedily from the Q-network given the state\n",
        "        state = np.copy(state)\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            q = self.policy(state)\n",
        "            q = q.to('cpu').detach().numpy()\n",
        "            action =  np.argmax(q)\n",
        "            return action.item()\n",
        "        #raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTN6ZVB1gLUe"
      },
      "source": [
        "### Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AhcdDZSef30u"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Useful wrappers taken from OpenAI (https://github.com/openai/baselines)\n",
        "\"\"\"\n",
        "from collections import deque\n",
        "import cv2\n",
        "import gym\n",
        "cv2.ocl.setUseOpenCL(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rdAZJk3Rf3vv"
      },
      "outputs": [],
      "source": [
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(\n",
        "                1, self.noop_max + 1\n",
        "            )  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(\n",
        "            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjPCwY6LgONx"
      },
      "source": [
        "### Train Atari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QEMMBULFf3rV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#import argparse\n",
        "#from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o6KBYQ39f3nA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********************************************************\n",
            "steps: 8737\n",
            "episodes: 10\n",
            "mean 100 episode reward: -20.1\n",
            "% time spent exploring: 91\n",
            "********************************************************\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [256]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/olvdejo/Desktop/COMS4047_Reinforcement Learning/Labs - Assignments/Lab 3/DQN_Lab.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     episode_rewards\u001b[39m.\u001b[39mappend(\u001b[39m0.0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     t \u001b[39m>\u001b[39m hyper_params[\u001b[39m\"\u001b[39m\u001b[39mlearning-starts\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39mand\u001b[39;00m t \u001b[39m%\u001b[39m hyper_params[\u001b[39m\"\u001b[39m\u001b[39mlearning-freq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m ):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     agent\u001b[39m.\u001b[39;49moptimise_td_loss()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     t \u001b[39m>\u001b[39m hyper_params[\u001b[39m\"\u001b[39m\u001b[39mlearning-starts\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39mand\u001b[39;00m t \u001b[39m%\u001b[39m hyper_params[\u001b[39m\"\u001b[39m\u001b[39mtarget-update-freq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m ):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     agent\u001b[39m.\u001b[39mupdate_target_network()\n",
            "\u001b[1;32m/Users/olvdejo/Desktop/COMS4047_Reinforcement Learning/Labs - Assignments/Lab 3/DQN_Lab.ipynb Cell 18\u001b[0m in \u001b[0;36mDQNAgent.optimise_td_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ddqn:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         _, max_nxt_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(nxt_states)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         max_nxt_q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget(nxt_states),\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/Users/olvdejo/Desktop/COMS4047_Reinforcement Learning/Labs - Assignments/Lab 3/DQN_Lab.ipynb Cell 18\u001b[0m in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     conv_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m],\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/olvdejo/Desktop/COMS4047_Reinforcement%20Learning/Labs%20-%20Assignments/Lab%203/DQN_Lab.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfully_connected(conv_out)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [256]"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    '''\n",
        "    parser = argparse.ArgumentParser(description='DQN Atari')\n",
        "    parser.add_argument('--load-checkpoint-file', type=str, default=None, \n",
        "                        help='Where checkpoint file should be loaded from (usually results/checkpoint.pth)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    # If you have a checkpoint file, spend less time exploring\n",
        "    if(args.load_checkpoint_file):\n",
        "        eps_start= 0.01\n",
        "    else:\n",
        "        eps_start= 1\n",
        "    '''\n",
        "        \n",
        "        \n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env\": \"PongNoFrameskip-v4\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),  # total number of steps to run the environment for\n",
        "        \"batch-size\": 256,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # number of steps before learning starts\n",
        "        \"learning-freq\": 5,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # number of iterations between every target network update\n",
        "        \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    assert \"NoFrameskip\" in hyper_params[\"env\"], \"Require environment with no frameskip\"\n",
        "    env = gym.make(hyper_params[\"env\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    # TODO Pick Gym wrappers to use\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    env = PyTorchFrame(env)\n",
        "    env = FrameStack(env,4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    #env.render()\n",
        "    #env = gym.wrappers.Monitor(env, './video/', video_callable=lambda episode_id: episode_id % 50 == 0, force=True)\n",
        "  \n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # TODO Create dqn agent\n",
        "    agent = DQNAgent(env.observation_space, \n",
        "                     env.action_space,\n",
        "                     replay_buffer, \n",
        "                     hyper_params[\"use-double-dqn\"],\n",
        "                     hyper_params[\"learning-rate\"],\n",
        "                     hyper_params[\"batch-size\"], \n",
        "                     hyper_params[\"discount-factor\"])\n",
        "    '''\n",
        "    if(args.load_checkpoint_file):\n",
        "        print(f\"Loading a policy - { args.load_checkpoint_file } \")\n",
        "        agent.policy.load_state_dict(\n",
        "        torch.load(args.load_checkpoint_file))\n",
        "    '''\n",
        "        \n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "    loss = [0.0]\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * (\n",
        "            hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"]\n",
        "        )\n",
        "        sample = random.random()\n",
        "        # TODO\n",
        "        #  select random action if sample is less equal than eps_threshold\n",
        "        # take step in env\n",
        "        # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "        # add reward to episode_reward\n",
        "\n",
        "        if sample > eps_threshold:\n",
        "            action = agent.act(np.array(state))\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        nxt_state, reward, done, _ = env.step(action)\n",
        "        agent.replay_memory.add(state, action, reward, nxt_state, float(done))\n",
        "        state = nxt_state\n",
        "\n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"learning-freq\"] == 0\n",
        "        ):\n",
        "            agent.optimise_td_loss()\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"target-update-freq\"] == 0\n",
        "        ):\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if (\n",
        "            done\n",
        "            and hyper_params[\"print-freq\"] is not None\n",
        "            and len(episode_rewards) % hyper_params[\"print-freq\"] == 0\n",
        "        ):\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n",
        "            '''\n",
        "            torch.save(agent.policy_q.state_dict(), f'checkpoint.pth')\n",
        "            np.savetxt('rewards_per_episode.csv', episode_rewards,\n",
        "                       delimiter=',', fmt='%1.3f')\n",
        "            '''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('rl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e464f6065abbcf73e173654187ddf7a6c58253bc8120564044a1282e6289ab0b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
